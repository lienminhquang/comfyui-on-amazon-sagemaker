#!/usr/bin/env python

# This file implements the scoring service shell. You don't necessarily need to modify it for various
# algorithms. It starts nginx and gunicorn with the correct configurations and then simply waits until
# gunicorn exits.
#
# The flask server is specified to be the app object in wsgi.py
#
# We set the following parameters:
#
# Parameter                Environment Variable              Default Value
# ---------                --------------------              -------------
# number of workers        INFERENCE_SERVER_WORKERS          number of CPU cores
# timeout                  INFERENCE_SERVER_TIMEOUT          70 seconds

import multiprocessing
import os
import signal
import subprocess
import sys

def run_nvidia_smi():
    try:
        # Execute the nvidia-smi command
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

        # Check for errors
        if result.returncode != 0:
            print("Error running nvidia-smi:")
            print(result.stderr)
        else:
            # Print the output of nvidia-smi
            print(result.stdout)
    except Exception as e:
        print(f"An error occurred: {e}")

# Call the function
run_nvidia_smi()

cpu_count = multiprocessing.cpu_count()

inference_server_timeout = os.environ.get("INFERENCE_SERVER_TIMEOUT", 70)
inference_server_workers = int(os.environ.get("INFERENCE_SERVER_WORKERS", cpu_count))


def sigterm_handler(nginx_pid, gunicorn_pid, app_pid):
    try:
        os.kill(nginx_pid, signal.SIGQUIT)
    except OSError:
        pass
    try:
        os.kill(gunicorn_pid, signal.SIGTERM)
    except OSError:
        pass
    try:
        os.kill(app_pid, signal.SIGTERM)
    except OSError:
        pass

    sys.exit(0)


def start_server():
    print("Starting the inference server with {} workers.".format(inference_server_workers))
    print("Listen to port 8080")

    # link the log streams to stdout/err so they will be logged to the container logs
    subprocess.check_call(["ln", "-sf", "/dev/stdout", "/var/log/nginx/access.log"])
    subprocess.check_call(["ln", "-sf", "/dev/stderr", "/var/log/nginx/error.log"])

    nginx = subprocess.Popen(["nginx", "-c", "/opt/program/nginx.conf"])
    gunicorn = subprocess.Popen(
        [
            "gunicorn",
            "--timeout",
            str(inference_server_timeout),
            "-k",
            "sync",
            "-b",
            "unix:/tmp/gunicorn.sock",
            "-w",
            str(inference_server_workers),
            "wsgi:app",
        ]
    )
    app = subprocess.Popen(["python3", "-u", "/opt/program/ComfyUI/main.py", "--listen", "127.0.0.1", "--port", "8188"])

    signal.signal(signal.SIGTERM, lambda a, b, c: sigterm_handler(nginx.pid, gunicorn.pid, app.pid))

    # If either subprocess exits, so do we.
    pids = set([nginx.pid, gunicorn.pid, app.pid])
    while True:
        pid, _ = os.wait()
        if pid in pids:
            break

    sigterm_handler(nginx.pid, gunicorn.pid, app.pid)
    print("Inference server exiting")


# The main routine just invokes the start function.

if __name__ == "__main__":
    start_server()